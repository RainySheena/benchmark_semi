{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd8cef83-3288-4905-8ba6-870c540fa3ad",
   "metadata": {},
   "source": [
    "### Import the necessary library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d07cc0fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'louvain'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscib\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlouvain\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'louvain'"
     ]
    }
   ],
   "source": [
    "import torch.utils.data\n",
    "import scanpy as sc\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial import distance\n",
    "import scipy.sparse\n",
    "import scib \n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from torch.distributions import  kl_divergence as kl\n",
    "torch.backends.cudnn.benchmark = True\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn import preprocessing\n",
    "import scipy\n",
    "import scib\n",
    "import louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3202fd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import scvi\n",
    "import anndata\n",
    "import pandas as pd\n",
    "from scipy.io import mmread\n",
    "from scipy.sparse import csr_matrix\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tempfile\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447b1881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "sc.set_figure_params(figsize=(6, 6), frameon=False)\n",
    "sns.set_theme()\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "save_dir = tempfile.TemporaryDirectory()\n",
    "\n",
    "%config InlineBackend.print_figure_kwargs={\"facecolor\": \"w\"}\n",
    "%config InlineBackend.figure_format=\"retina\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Suppress specific ImportWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ImportWarning, message=\".*AltairImportHook.find_spec() not found; falling back to find_module.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8eed628",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/rpy2/robjects/pandas2ri.py:368: DeprecationWarning: The global conversion available with activate() is deprecated and will be removed in the next major release. Use a local converter.\n",
      "  warnings.warn('The global conversion available with activate() '\n",
      "/opt/anaconda3/lib/python3.12/site-packages/rpy2/robjects/numpy2ri.py:252: DeprecationWarning: The global conversion available with activate() is deprecated and will be removed in the next major release. Use a local converter.\n",
      "  warnings.warn('The global conversion available with activate() '\n"
     ]
    }
   ],
   "source": [
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects import pandas2ri\n",
    "pandas2ri.activate()\n",
    "\n",
    "#Perhaps require the library relocation (to the library contains kBET and lisi library)\n",
    "robjects.r('.libPaths(c(\"current path\", \"library location\"))')\n",
    "\n",
    "rscript = '''\n",
    "library(kBET)\n",
    "# library(lisi)\n",
    "'''\n",
    "robjects.r(rscript)\n",
    "kbet = robjects.r('kBET')\n",
    "# lisi = robjects.r['compute_lisi']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045491cd-6b4b-49cf-b07f-2739872f1329",
   "metadata": {},
   "source": [
    "### read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dca45c4-5236-48b5-b785-0e629a8f1a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 30302 × 36162\n",
       "    obs: 'batch', 'sample', 'macaque_id', 'nGene', 'nTranscripts', 'cluster', 'region', 'class', 'cell_type', 'BATCH'\n",
       "    var: 'gene_name'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the prediction results to adata\n",
    "import anndata\n",
    "adata = anndata.read_h5ad(\"../macaque_raw.h5ad\")\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0db3e33-f564-40b6-b3e4-df20fcc168c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 30302 × 36162\n",
       "    obs: 'batch', 'sample', 'macaque_id', 'nGene', 'nTranscripts', 'cluster', 'region', 'class', 'cell_type', 'BATCH'\n",
       "    var: 'gene_name'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc665fe-136d-4e33-b694-6331d48f4c34",
   "metadata": {},
   "source": [
    "## Print the normal metrics (for scib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf54396-5e2e-4759-a194-aa044659e049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scib\n",
    "\n",
    "def calculate_metrics_for_embeddings(file_path, adata, embedding_keys, batch_key='batch', label_key='cell_type'):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    combined_embeddings = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "    # Ensure indices match\n",
    "    combined_embeddings = combined_embeddings.loc[adata.obs_names]\n",
    "\n",
    "    # Split the DataFrame into separate DataFrames for each embedding\n",
    "    embeddings_dict = {key: combined_embeddings.filter(like=key) for key in embedding_keys}\n",
    "\n",
    "    # Assign the embeddings back to adata.obsm\n",
    "    for key, df in embeddings_dict.items():\n",
    "        adata.obsm[key] = df.values\n",
    "\n",
    "    # Initialize an empty list to store results\n",
    "    results = []\n",
    "\n",
    "    # Loop through each embedding and calculate metrics\n",
    "    for key in embedding_keys:\n",
    "        print(f\"File: {file_path}, Embedding: {key}\")\n",
    "        sc.pp.neighbors(adata, use_rep=key)\n",
    "        scib.me.cluster_optimal_resolution(adata, cluster_key=\"cluster\", label_key=label_key)\n",
    "        metrics = scib.me.metrics(\n",
    "            adata, adata_int=adata, \n",
    "            silhouette_=True, graph_conn_=True, \n",
    "            ari_=True, nmi_=True, isolated_labels_=True, \n",
    "            isolated_labels_f1_=True,\n",
    "            isolated_labels_asw_=True, \n",
    "            batch_key=batch_key, label_key=label_key, \n",
    "            embed=key\n",
    "        )\n",
    "        \n",
    "        for metric, score in metrics.items():\n",
    "            # Store results with identifying information\n",
    "            result_entry = {\n",
    "                'file_path': file_path,\n",
    "                'embedding_key': key,\n",
    "                'metric': metric,\n",
    "                'score': score\n",
    "            }\n",
    "            results.append(result_entry)\n",
    "            \n",
    "            # Print the file path, embedding key, and result\n",
    "            print(f\"File: {file_path}, Embedding: {key}, Metric: {metric}, Score: {score}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1af539f-9b03-42d5-93d8-5d348ebd5fc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_paths = [\n",
    "    # \"../embeddings/full_annotated_supervised_macaque.csv\",\n",
    "    # \"../embeddings/partially_annotated_batches_macaque_30.csv\",\n",
    "    # \"../embeddings/partially_annotated_batches_macaque_50.csv\",\n",
    "    # \"../embeddings/partially_annotated_batches_macaque_70.csv\",\n",
    "    # \"../embeddings/randomly_wrong_macaque_30.csv\",\n",
    "    # \"../embeddings/randomly_wrong_macaque_50.csv\",\n",
    "    # \"../embeddings/randomly_wrong_macaque_70.csv\",\n",
    "    \"../embeddings/mixing_at_edge_macaque_30.csv\",\n",
    "    \"../embeddings/mixing_at_edge_macaque_50.csv\",\n",
    "    \"../embeddings/mixing_at_edge_macaque_70.csv\"\n",
    "]\n",
    "\n",
    "# Initialize the list of embedding keys to use\n",
    "embedding_keys = [\"X_scDREAMER\", \"X_ItClust\"]\n",
    "\n",
    "# Prepare a list to collect all results\n",
    "all_results = []\n",
    "\n",
    "# Loop through each file and calculate metrics\n",
    "for file_path in file_paths:\n",
    "    results = calculate_metrics_for_embeddings(file_path, adata, embedding_keys)\n",
    "    all_results.extend(results)\n",
    "\n",
    "# Now `all_results` contains metrics for all embedding files with identifying information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5a9373-1da0-43b6-91b2-31407946f853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the nested dictionaries and extract desired scores\n",
    "results = []\n",
    "for entry in all_results:\n",
    "    file_path = entry['file_path']\n",
    "    embedding_key = entry['embedding_key']\n",
    "    score_series = entry['score']\n",
    "    for metric, value in score_series.items():\n",
    "        # Create an entry for each score\n",
    "        results.append({\n",
    "            'file_path': file_path,\n",
    "            'embedding_key': embedding_key,\n",
    "            'metric': metric,\n",
    "            'score': value\n",
    "        })\n",
    "\n",
    "# Convert results list to DataFrame\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdcd3d5-536a-443b-b3d2-5772813c4775",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a89f17-62b5-428d-afb8-5c296e3c2b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('../metrics/macaque_scib_metrics_2.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db745ea",
   "metadata": {},
   "source": [
    "## Print the overall metrics for TPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69438078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive and true positive cells defined in iMAP\n",
    "def positive_true_positive(adata, batch_key='batch', celltype_key='cell_type', use_raw=False,k1=20, k2=100, tp_thr=3., distance='cosine', embed='X_pca'):\n",
    "    celltype_list = adata.obs[celltype_key]\n",
    "    batch_list = adata.obs[batch_key]\n",
    "\n",
    "    temp_c = adata.obs[celltype_key].value_counts()\n",
    "    temp_b = pd.crosstab(adata.obs[celltype_key], adata.obs[batch_key])\n",
    "    temp_b_prob = temp_b.divide(temp_b.sum(1), axis=0)\n",
    "    \n",
    "    if use_raw:\n",
    "        if isinstance(adata.X, scipy.sparse.csr.csr_matrix):\n",
    "            X = adata.X.todense()\n",
    "        else:\n",
    "            X = adata.X\n",
    "    else:\n",
    "        X = adata.obsm[embed]\n",
    "    if distance == 'cosine':\n",
    "        X = preprocessing.normalize(X, axis=1)\n",
    "\n",
    "    t1 = KDTree(X)\n",
    "\n",
    "    p_list = []\n",
    "    tp_list = []\n",
    "\n",
    "    for cell in range(len(X)):\n",
    "\n",
    "        # Discriminate positive cells\n",
    "        neig1 = min(k1, temp_c[celltype_list[cell]])\n",
    "        NNs = t1.query(X[cell].reshape(1,-1), neig1+1, return_distance=False)[0, 1:]\n",
    "        c_NN = celltype_list[NNs]\n",
    "        true_rate = sum(c_NN == celltype_list[cell])/neig1\n",
    "        if true_rate > 0.5:\n",
    "            p_list.append(True)\n",
    "        else:\n",
    "            p_list.append(False)\n",
    "\n",
    "        # Discriminate true positive cells\n",
    "        if p_list[cell] == True:\n",
    "            neig2 = min(k2, temp_c[celltype_list[cell]])\n",
    "            NNs = t1.query(X[cell].reshape(1,-1), neig2, return_distance=False)[0]\n",
    "            NNs_c = celltype_list[NNs]\n",
    "            NNs_i = NNs_c == celltype_list[cell]\n",
    "            NNs = NNs[NNs_i] # get local neighbors that are from the same cell type\n",
    "            neig2 = len(NNs)\n",
    "            NNs_b = batch_list[NNs]\n",
    "\n",
    "            max_b = 0\n",
    "            b_prob = temp_b_prob.loc[celltype_list[cell]]\n",
    "            for b in set(batch_list):\n",
    "                if b_prob[b] > 0 and b_prob[b] < 1:\n",
    "                    p_b = sum(NNs_b == b)\n",
    "                    stat_b = abs(p_b - neig2*b_prob[b]) / np.sqrt(neig2*b_prob[b]*(1-b_prob[b]))\n",
    "                    max_b = max(max_b, stat_b)\n",
    "            if max_b <= tp_thr:\n",
    "                tp_list.append(True)\n",
    "            else:\n",
    "                tp_list.append(False)\n",
    "        else:\n",
    "            tp_list.append(False)\n",
    "\n",
    "    pos_rate = sum(p_list)/len(p_list)\n",
    "    truepos_rate = sum(tp_list)/len(tp_list)\n",
    "    return pos_rate, truepos_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fb595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "\n",
    "def evaluate_embeddings(adata, file_paths, embedding_keys, batch_key='batch', celltype_key='cell_type', use_raw=False, k1=20, k2=100, tp_thr=3.0, distance='cosine'):\n",
    "    results = []  # Initialize results as a list\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        combined_embeddings = pd.read_csv(file_path, index_col=0)\n",
    "        \n",
    "        # Ensure indices match\n",
    "        combined_embeddings = combined_embeddings.loc[adata.obs_names]\n",
    "        \n",
    "        # Split the DataFrame into separate DataFrames for each embedding\n",
    "        embeddings_dict = {key: combined_embeddings.filter(like=key) for key in embedding_keys}\n",
    "        \n",
    "        # Assign the embeddings back to adata.obsm\n",
    "        for key, df in embeddings_dict.items():\n",
    "            adata.obsm[key] = df.values\n",
    "            \n",
    "            # Evaluate embedding\n",
    "            pos_rate, truepos_rate = positive_true_positive(\n",
    "                adata,\n",
    "                batch_key=batch_key,\n",
    "                celltype_key=celltype_key,\n",
    "                use_raw=use_raw,\n",
    "                k1=k1,\n",
    "                k2=k2,\n",
    "                tp_thr=tp_thr,\n",
    "                distance=distance,\n",
    "                embed=key\n",
    "            )\n",
    "            \n",
    "            # Store results with identifying information\n",
    "            result_entry = {\n",
    "                'file_path': file_path,\n",
    "                'embedding_key': key,\n",
    "                'pos_rate': pos_rate,\n",
    "                'truepos_rate': truepos_rate\n",
    "            }\n",
    "            results.append(result_entry)\n",
    "            \n",
    "            # Print the file path, embedding key, and result\n",
    "            print(f\"File Path: {file_path}, Embedding Key: {key}, pos_rate: {pos_rate}, truepos_rate: {truepos_rate}\")\n",
    "    \n",
    "    # Convert results list to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792376a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\n",
    "    \"../embeddings/full_annotated_supervised_macaque.csv\",\n",
    "    \"../embeddings/partially_annotated_batches_macaque_30.csv\",\n",
    "    \"../embeddings/partially_annotated_batches_macaque_50.csv\",\n",
    "    \"../embeddings/partially_annotated_batches_macaque_70.csv\",\n",
    "    \"../embeddings/mixing_at_edge_macaque_30.csv\",\n",
    "    \"../embeddings/mixing_at_edge_macaque_50.csv\",\n",
    "    \"../embeddings/mixing_at_edge_macaque_70.csv\",\n",
    "    \"../embeddings/randomly_wrong_macaque_30.csv\",\n",
    "    \"../embeddings/randomly_wrong_macaque_50.csv\",\n",
    "    \"../embeddings/randomly_wrong_macaque_70.csv\"\n",
    "]\n",
    "\n",
    "# Initialize the list of embedding keys to use\n",
    "embedding_keys = [\"X_scDREAMER\", \"X_ItClust\"]\n",
    "\n",
    "\n",
    "results_df = evaluate_embeddings(adata, file_paths, embedding_keys)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "results_df.to_csv('../metrics/macaque_true_positive_true_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9a1ba7-409c-43bd-b04c-13d33a030215",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb22d97-b58f-4c33-bf57-f0e11aa8b6f7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Print the overall metrics for kBET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a461fa1-ade1-4ec2-9997-eae9bed5619a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import scib  \n",
    "import scipy.sparse as sp\n",
    "\n",
    "# teach all csr_matrix objects that `.A` means “dense array”\n",
    "sp.csr_matrix.A = property(lambda self: self.toarray())\n",
    "\n",
    "\n",
    "def evaluate_embeddings(adata, file_paths, embedding_keys, batch_key='batch', celltype_key='cell_type'):\n",
    "    results = []  # Initialize results as a list\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        combined_embeddings = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "        # Ensure indices match\n",
    "        combined_embeddings = combined_embeddings.loc[adata.obs_names]\n",
    "\n",
    "        # Split the DataFrame into separate DataFrames for each embedding\n",
    "        embeddings_dict = {key: combined_embeddings.filter(like=key) for key in embedding_keys}\n",
    "\n",
    "        # Assign the embeddings back to adata.obsm\n",
    "        for key, df in embeddings_dict.items():\n",
    "            adata.obsm[key] = df.values\n",
    "\n",
    "            # Calculate kBET score\n",
    "            kbet_score = scib.metrics.kBET(\n",
    "                adata,\n",
    "                batch_key=batch_key,\n",
    "                label_key=celltype_key,\n",
    "                type_=None,\n",
    "                embed=key,\n",
    "                scaled=True,\n",
    "                verbose=False,\n",
    "            )\n",
    "\n",
    "            # Store results with identifying information\n",
    "            result_entry = {\n",
    "                'file_path': file_path,\n",
    "                'embedding_key': key,\n",
    "                'kbet_score': kbet_score\n",
    "            }\n",
    "            results.append(result_entry)\n",
    "\n",
    "            # Print the file path, embedding key, and result\n",
    "            print(f\"File Path: {file_path}, Embedding Key: {key}, kBET Score: {kbet_score}\")\n",
    "\n",
    "    # Convert results list to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb158006",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\n",
    "    # \"../embeddings/full_annotated_supervised_macaque.csv\",\n",
    "    # \"../embeddings/partially_annotated_batches_macaque_30.csv\",\n",
    "    # \"../embeddings/partially_annotated_batches_macaque_50.csv\",\n",
    "    # \"../embeddings/partially_annotated_batches_macaque_70.csv\",\n",
    "    # \"../embeddings/mixing_at_edge_macaque_30.csv\",\n",
    "    # \"../embeddings/mixing_at_edge_macaque_50.csv\",\n",
    "    # \"../embeddings/mixing_at_edge_macaque_70.csv\",\n",
    "    \"../embeddings/randomly_wrong_macaque_30.csv\",\n",
    "    \"../embeddings/randomly_wrong_macaque_50.csv\",\n",
    "    \"../embeddings/randomly_wrong_macaque_70.csv\"\n",
    "]\n",
    "\n",
    "# Initialize the list of embedding keys to use\n",
    "embedding_keys = [\"X_scANVI\", \"X_scgen\", \"X_stacas\", \"X_scDREAMER\", \"X_itclust\"]\n",
    "\n",
    "\n",
    "results_df = evaluate_embeddings(adata, file_paths, embedding_keys)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "results_df.to_csv('../metrics/macaque_kbet_scores_randomly_wrong.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57df3bd0-552f-4114-8c6e-45d73cf89276",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\n",
    "    \"../embeddings/full_annotated_supervised_macaque.csv\",\n",
    "    \"../embeddings/partially_annotated_batches_macaque_30.csv\",\n",
    "    \"../embeddings/partially_annotated_batches_macaque_50.csv\",\n",
    "    \"../embeddings/partially_annotated_batches_macaque_70.csv\",\n",
    "    \"../embeddings/mixing_at_edge_macaque_30.csv\",\n",
    "    \"../embeddings/mixing_at_edge_macaque_50.csv\",\n",
    "    \"../embeddings/mixing_at_edge_macaque_70.csv\",\n",
    "    \"../embeddings/randomly_wrong_macaque_30.csv\",\n",
    "    \"../embeddings/randomly_wrong_macaque_50.csv\",\n",
    "    \"../embeddings/randomly_wrong_macaque_70.csv\"\n",
    "]\n",
    "\n",
    "# Initialize the list of embedding keys to use\n",
    "embedding_keys = [\"X_scDREAMER\", \"X_ItClust\"]\n",
    "\n",
    "\n",
    "results_df = evaluate_embeddings(adata, file_paths, embedding_keys)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "results_df.to_csv('../metrics/macaque_kbet_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9bcef3-4b1a-413c-9cbb-762b69cdf7ea",
   "metadata": {},
   "source": [
    "## Print the overall metrics for LISI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaf690e-2f66-47bd-889b-0a236e3d8fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LISI\n",
    "def calculate_LISI(adata, labels=None, total_cells=None, batch_key='batch', celltype_key='celltype', embed = 'X_pca'):\n",
    "    if celltype_key is None:\n",
    "        # Calculate bLISI\n",
    "        lisi_b = 0\n",
    "        lisi_c = lisi_f1 = np.nan\n",
    "        if adata.shape[0] < 90:\n",
    "            perplexity = int(adata.shape[0]/6)\n",
    "        else:\n",
    "            perplexity = 30\n",
    "        lisi_res = lisi(adata.obsm[embed], adata.obs, batch_key, perplexity=perplexity)\n",
    "        lisi_b = (np.mean(lisi_res)[batch_key]-1.)/(len(set(adata.obs[batch_key]))-1.)\n",
    "    else:\n",
    "        # Calculate 1-cLISI\n",
    "        lisi_res = lisi(adata.obsm[embed], adata.obs, celltype_key)\n",
    "        lisi_c = 1 - (np.mean(lisi_res)-1.)/(len(set(adata.obs[celltype_key]))-1.)\n",
    "\n",
    "        # Calculate bLISI\n",
    "        lisi_b = 0\n",
    "        for label in labels:\n",
    "            adata_sub = adata[adata.obs[celltype_key] == label]\n",
    "            if adata_sub.shape[0] < 90:\n",
    "                perplexity = int(adata_sub.shape[0]/6)\n",
    "            else:\n",
    "                perplexity = 30\n",
    "            lisi_res = lisi(adata_sub.obsm[embed], adata_sub.obs, batch_key, perplexity=perplexity)\n",
    "            lisi_batch = (np.mean(lisi_res)-1.)/(len(set(adata_sub.obs[batch_key]))-1.)\n",
    "            lisi_b += lisi_batch*adata_sub.shape[0]\n",
    "        lisi_b /= total_cells\n",
    "        lisi_c = lisi_c.item()\n",
    "        lisi_b = lisi_b.item()  # This will print only the value without the data type\n",
    "\n",
    "        # Calcualte F1 score\n",
    "        lisi_f1 = (2*lisi_c*lisi_b)/(lisi_c + lisi_b)\n",
    "    \n",
    "    return lisi_c, lisi_b, lisi_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32509114-e30a-43c9-8621-049e9dbf19f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_embeddings(adata, file_paths, embedding_keys, batch_key='batch', celltype_key='cell_type'):\n",
    "    results = []  # Initialize results as a list\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        combined_embeddings = pd.read_csv(file_path, index_col=0)\n",
    "        \n",
    "        # Ensure indices match\n",
    "        combined_embeddings = combined_embeddings.loc[adata.obs_names]\n",
    "        \n",
    "        # Split the DataFrame into separate DataFrames for each embedding\n",
    "        embeddings_dict = {key: combined_embeddings.filter(like=key) for key in embedding_keys}\n",
    "        \n",
    "        # Assign the embeddings back to adata.obsm\n",
    "        for key, df in embeddings_dict.items():\n",
    "            adata.obsm[key] = df.values\n",
    "            \n",
    "            # Evaluate embedding\n",
    "            lisi_c, lisi_b, lisi_f1 = calculate_LISI(\n",
    "                adata, labels=adata.obs[celltype_key].unique(), total_cells=adata.shape[0], batch_key=batch_key, celltype_key=celltype_key, embed=key\n",
    "            )\n",
    "            \n",
    "            # Store results with identifying information\n",
    "            result_entry = {\n",
    "                'file_path': file_path,\n",
    "                'embedding_key': key,\n",
    "                'lisi_c': lisi_c,\n",
    "                'lisi_b': lisi_b,\n",
    "                'lisi_f1': lisi_f1\n",
    "            }\n",
    "            results.append(result_entry)\n",
    "            \n",
    "            # Print the file path, embedding key, and result\n",
    "            print(f\"File Path: {file_path}, Embedding Key: {key}, LISI_c: {lisi_c}, LISI_b: {lisi_b}, LISI_f1: {lisi_f1}\")\n",
    "    \n",
    "    # Convert results list to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3190a042-44bd-462d-aeef-17cee3bbc983",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\n",
    "    \"../embeddings/full_annotated_supervised_macaque.csv\",\n",
    "    \"../embeddings/partially_annotated_batches_macaque_30.csv\",\n",
    "    \"../embeddings/partially_annotated_batches_macaque_50.csv\",\n",
    "    \"../embeddings/partially_annotated_batches_macaque_70.csv\",\n",
    "    \"../embeddings/mixing_at_edge_macaque_30.csv\",\n",
    "    \"../embeddings/mixing_at_edge_macaque_50.csv\",\n",
    "    \"../embeddings/mixing_at_edge_macaque_70.csv\",\n",
    "    \"../embeddings/randomly_wrong_macaque_30.csv\",\n",
    "    \"../embeddings/randomly_wrong_macaque_50.csv\",\n",
    "    \"../embeddings/randomly_wrong_macaque_70.csv\"\n",
    "]\n",
    "\n",
    "# Initialize the list of embedding keys to use\n",
    "embedding_keys = [\"X_scDREAMER\", \"X_ItClust\"]\n",
    "\n",
    "results_df = evaluate_embeddings(adata, file_paths, embedding_keys)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "results_df.to_csv('../metrics/macaque_lisi_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9b96ae-ad22-4898-9f0d-9bb2c08551a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61264a74-03f6-4aa2-8d6a-2df6f6e07bed",
   "metadata": {},
   "source": [
    "# Combine the results for each embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcab771-2679-4916-a605-1e3d9b95f8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读入Excel文件\n",
    "file_path = './macaque_metrics.csv'  # 替换为你的文件路径\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 处理数据，对每一个setting生成一个表\n",
    "settings = df['type'].unique()\n",
    "output_file = pd.ExcelWriter('./macaque_metrics_grouped.xlsx')\n",
    "\n",
    "# 创建一个Excel writer对象，利用openpyxl引擎\n",
    "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "    for setting in settings:\n",
    "        setting_df = df[df['type'] == setting]\n",
    "\n",
    "        # 创建透视表，将行设为method，列设为metric，score为值\n",
    "        pivot_table = setting_df.pivot(index='method', columns='metric', values='score')\n",
    "\n",
    "        # 写入新的Excel sheet中\n",
    "        pivot_table.to_excel(writer, sheet_name=setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca48dbb-f3b3-4034-a4e1-16decb920aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1FRD5YoE7CE0kJ3E0RUMMPa4NRYKklAy9",
     "timestamp": 1720314928051
    },
    {
     "file_id": "1pkZQvEPrFT08w67Q1949s-8NXcKibnXE",
     "timestamp": 1720229381943
    },
    {
     "file_id": "18vNKd-L7InkqF1oeT_AeI-C2WqPrc7fg",
     "timestamp": 1720142103618
    }
   ]
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
